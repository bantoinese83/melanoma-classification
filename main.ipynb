{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6a76be509c266e71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import joblib\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from halo import Halo\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from loguru import logger\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from data_visualization import plot_target_distribution, plot_age_distribution, plot_feature_correlation\n",
    "\n",
    "# Importing configuration settings\n",
    "import config\n"
   ],
   "id": "475a2de0ab786b40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_data(train_path, test_path):\n",
    "    spinner = Halo(text='Loading data...', spinner='dots')\n",
    "    spinner.start()\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        spinner.succeed('Data loaded successfully')\n",
    "        return train_df, test_df\n",
    "    except Exception as e:\n",
    "        spinner.fail('Failed to load data')\n",
    "        logger.error(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "def preprocess_data(df, is_train=True):\n",
    "    spinner = Halo(text='Preprocessing data...', spinner='dots')\n",
    "    spinner.start()\n",
    "    df['age_group'] = pd.cut(df['age_approx'], bins=config.AGE_GROUP_BINS, labels=config.AGE_GROUP_LABELS)\n",
    "    df['anatom_site_category'] = df['anatom_site_general_challenge'].astype('category').cat.codes\n",
    "    df['sex_site_interaction'] = df['sex'].astype(str) + '_' + df['anatom_site_general_challenge'].astype(str)\n",
    "\n",
    "    for feature in config.NUMERIC_FEATURES:\n",
    "        if feature in df.columns:\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            df[feature] = imputer.fit_transform(df[[feature]]).ravel()\n",
    "        else:\n",
    "            logger.warning(f\"Warning: {feature} is not present in the DataFrame\")\n",
    "\n",
    "    for feature in config.CATEGORICAL_FEATURES:\n",
    "        if feature in df.columns:\n",
    "            df[feature] = df[feature].astype(str)\n",
    "            le = LabelEncoder()\n",
    "            df[feature] = le.fit_transform(df[feature])\n",
    "\n",
    "    for feature in config.CATEGORICAL_FEATURES:\n",
    "        if feature in df.columns:\n",
    "            imputer = SimpleImputer(strategy='most_frequent')\n",
    "            df[feature] = imputer.fit_transform(df[[feature]]).ravel()\n",
    "        else:\n",
    "            logger.warning(f\"Warning: {feature} is not present in the DataFrame\")\n",
    "\n",
    "    for feature in config.CATEGORICAL_FEATURES + ['age_group', 'anatom_site_category', 'sex_site_interaction']:\n",
    "        if feature in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[feature] = le.fit_transform(df[feature])\n",
    "        else:\n",
    "            logger.warning(f\"Warning: '{feature}' is not present in the DataFrame\")\n",
    "\n",
    "    if is_train:\n",
    "        df = df.drop(columns=[col for col in config.DROP_COLUMNS_TRAIN if col in df.columns])\n",
    "        X = df.drop(columns=['target'])\n",
    "        y = df['target']\n",
    "        spinner.succeed('Training data preprocessed')\n",
    "        return X, y\n",
    "    else:\n",
    "        df = df.drop(columns=[col for col in config.DROP_COLUMNS_TEST if col in df.columns])\n",
    "        spinner.succeed('Test data preprocessed')\n",
    "        return df\n"
   ],
   "id": "6f03d1f2adf59cc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a study with a custom name\n",
    "study = optuna.create_study(direction=config.DIRECTION, study_name=config.STUDY_NAME)\n",
    "\n",
    "\n",
    "def objective(trial, X, y):\n",
    "    xgb_params = {\n",
    "        'n_estimators': trial.suggest_int('xgb_n_estimators', *config.XGB_PARAM_RANGE['n_estimators']),\n",
    "        'max_depth': trial.suggest_int('xgb_max_depth', *config.XGB_PARAM_RANGE['max_depth']),\n",
    "        'learning_rate': trial.suggest_float('xgb_learning_rate', *config.XGB_PARAM_RANGE['learning_rate']),\n",
    "        'subsample': trial.suggest_float('xgb_subsample', *config.XGB_PARAM_RANGE['subsample']),\n",
    "        'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', *config.XGB_PARAM_RANGE['colsample_bytree'])\n",
    "    }\n",
    "\n",
    "    rf_params = {\n",
    "        'n_estimators': trial.suggest_int('rf_n_estimators', *config.RF_PARAM_RANGE['n_estimators']),\n",
    "        'max_depth': trial.suggest_int('rf_max_depth', *config.RF_PARAM_RANGE['max_depth']),\n",
    "        'min_samples_split': trial.suggest_int('rf_min_samples_split', *config.RF_PARAM_RANGE['min_samples_split']),\n",
    "        'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', *config.RF_PARAM_RANGE['min_samples_leaf']),\n",
    "        'bootstrap': trial.suggest_categorical('rf_bootstrap', config.RF_PARAM_RANGE['bootstrap'])\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        model_pipeline = ImbPipeline(steps=[\n",
    "            ('smote', SMOTE(random_state=config.SMOTE_RANDOM_STATE)),\n",
    "            ('rfecv', RFECV(estimator=XGBClassifier(**xgb_params), step=1, cv=5, scoring='accuracy')),\n",
    "            ('classifier', VotingClassifier(estimators=[\n",
    "                ('xgb', XGBClassifier(**xgb_params)),\n",
    "                ('rf', RandomForestClassifier(**rf_params))\n",
    "            ], voting='soft'))\n",
    "        ])\n",
    "\n",
    "        cross_val_results = cross_val_score(model_pipeline, X, y, cv=5, scoring='f1')\n",
    "        logger.info(f\"Cross-validation F1 scores: {cross_val_results}\")\n",
    "\n",
    "        return np.mean(cross_val_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Trial failed with parameters: {trial.params}\")\n",
    "        logger.error(f\"Error: {e}\")\n",
    "        return 0.0\n"
   ],
   "id": "b6435f455fd5f8a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_model(X_train, y_train, n_trials=config.N_TRIALS):\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=n_trials)\n",
    "    best_params = study.best_params\n",
    "\n",
    "    logger.info(f\"Best trial F1 score: {study.best_value}\")\n",
    "    logger.info(f\"Best trial parameters: {best_params}\")\n",
    "\n",
    "    xgb_best_params = {\n",
    "        'n_estimators': best_params['xgb_n_estimators'],\n",
    "        'max_depth': best_params['xgb_max_depth'],\n",
    "        'learning_rate': best_params['xgb_learning_rate'],\n",
    "        'subsample': best_params['xgb_subsample'],\n",
    "        'colsample_bytree': best_params['xgb_colsample_bytree']\n",
    "    }\n",
    "\n",
    "    rf_best_params = {\n",
    "        'n_estimators': best_params['rf_n_estimators'],\n",
    "        'max_depth': best_params['rf_max_depth'],\n",
    "        'min_samples_split': best_params['rf_min_samples_split'],\n",
    "        'min_samples_leaf': best_params['rf_min_samples_leaf'],\n",
    "        'bootstrap': best_params['rf_bootstrap']\n",
    "    }\n",
    "\n",
    "    model_pipeline = ImbPipeline(steps=[\n",
    "        ('smote', SMOTE(random_state=config.SMOTE_RANDOM_STATE)),\n",
    "        ('rfecv', RFECV(estimator=XGBClassifier(**xgb_best_params), step=1, cv=5, scoring='accuracy')),\n",
    "        ('classifier', VotingClassifier(estimators=[\n",
    "            ('xgb', XGBClassifier(**xgb_best_params)),\n",
    "            ('rf', RandomForestClassifier(**rf_best_params))\n",
    "        ], voting='soft'))\n",
    "    ])\n",
    "\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    return model_pipeline\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    try:\n",
    "        cross_val_results = cross_val_score(model, X_val, y_val, cv=5, scoring='f1')\n",
    "        logger.info(f\"Cross-validation F1 scores on validation set: {cross_val_results}\")\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "        cr = classification_report(y_val, y_pred)\n",
    "\n",
    "        return accuracy, f1, cm, cr\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Evaluation failed with error: {e}\")\n",
    "        return 0.0, 0.0, np.array([[0, 0], [0, 0]]), \"Evaluation failed\"\n"
   ],
   "id": "f4f2c4b589bc8d9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_model(model, path):\n",
    "    try:\n",
    "        joblib.dump(model, path)\n",
    "        logger.info('Model saved successfully')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save model: {e}\")\n",
    "\n",
    "\n",
    "def analyze_feature_importance(fitted_model, training_data):\n",
    "    spinner = Halo(text='Analyzing feature importance...', spinner='dots')\n",
    "    spinner.start()\n",
    "    importances = fitted_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    logger.info(f\"Number of features in training data: {training_data.shape[1]}\")\n",
    "    logger.info(f\"Number of importances: {len(importances)}\")\n",
    "    logger.info(\"Feature ranking:\")\n",
    "    for i in range(min(training_data.shape[1], len(importances))):\n",
    "        logger.info(f\"{i + 1}. feature {indices[i]} ({importances[indices[i]]})\")\n",
    "    spinner.succeed('Feature importance analysis completed')\n",
    "\n",
    "\n",
    "def analyze_trials(study_trials):\n",
    "    trials = study_trials.trials\n",
    "    trial_data = [(trial.number, trial.value, trial.params) for trial in trials if trial.value is not None]\n",
    "\n",
    "    df = pd.DataFrame(trial_data, columns=['trial_number', 'f1_score', 'params'])\n",
    "    df['mean_f1_score'] = df['f1_score'].apply(np.mean)\n",
    "    df['std_f1_score'] = df['f1_score'].apply(np.std)\n",
    "\n",
    "    df_sorted = df.sort_values(by='mean_f1_score', ascending=False)\n",
    "    best_trial = df_sorted.iloc[0]\n",
    "    worst_trial = df_sorted.iloc[-1]\n",
    "\n",
    "    print(\"Best Trial:\")\n",
    "    print(best_trial)\n",
    "    print(\"Worst Trial:\")\n",
    "    print(worst_trial)\n",
    "\n",
    "    return df_sorted\n"
   ],
   "id": "d0737d725a8d4b79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    train_df, test_df = load_data(config.TRAIN_PATH, config.TEST_PATH)\n",
    "\n",
    "    plot_target_distribution(train_df)\n",
    "    plot_age_distribution(train_df)\n",
    "    plot_feature_correlation(train_df)\n",
    "\n",
    "    try:\n",
    "        X_train, y_train = preprocess_data(train_df, is_train=True)\n",
    "        X_test = preprocess_data(test_df, is_train=False)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Data preprocessing failed with error: {e}\")\n",
    "        return\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2,\n",
    "                                                                              random_state=42)\n",
    "\n",
    "    model_pipeline = train_model(X_train_split, y_train_split, n_trials=config.N_TRIALS)\n",
    "\n",
    "    accuracy, f1, cm, cr = evaluate_model(model_pipeline, X_val_split, y_val_split)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"Classification Report:\\n\", cr)\n",
    "\n",
    "    save_model(model_pipeline, config.MODEL_SAVE_PATH)\n",
    "\n",
    "    best_xgb_model = model_pipeline.named_steps['classifier'].estimators_[0]\n",
    "    analyze_feature_importance(best_xgb_model, X_train_split)\n",
    "\n",
    "    try:\n",
    "        test_predictions = model_pipeline.predict(X_test)\n",
    "        output = pd.DataFrame({'Id': test_df.index, 'target': test_predictions})\n",
    "        output.to_csv(config.TEST_PREDICTIONS_PATH, index=False)\n",
    "        logger.info('Test predictions saved successfully')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to predict on test set: {e}\")\n",
    "\n",
    "    # Analyze trials\n",
    "    trial_results = analyze_trials(study)\n",
    "    print(trial_results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "10a3a6e4ad898096"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
